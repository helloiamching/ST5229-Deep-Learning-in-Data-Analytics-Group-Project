{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1bf5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We include some files from the author's repo \n",
    "https://github.com/zhanghang1989/ResNeSt/tree/master/resnest/torch/models\n",
    "we change the path for some files to make it consistent with ours, like \n",
    "from .resnet import * -> from resnet import*\n",
    "'''\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from resnet import ResNet, Bottleneck\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, name, model, criterion, optimizer, device):\n",
    "        \"\"\"\n",
    "        Initializes the Trainer.\n",
    "\n",
    "        Args:\n",
    "            model (nn.Module): The PyTorch model to train.\n",
    "            criterion (nn.Module): The loss function.\n",
    "            optimizer (torch.optim.Optimizer): The optimizer.\n",
    "            device (torch.device): The device to train on (e.g., 'cuda' or 'cpu').\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.model = model.to(device)\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def train_epoch(self, dataloader):\n",
    "        \"\"\"\n",
    "        Trains the model for one epoch.\n",
    "\n",
    "        Args:\n",
    "            dataloader (DataLoader): The DataLoader for the training set.\n",
    "\n",
    "        Returns:\n",
    "            float: The average training loss for the epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = len(dataloader)\n",
    "\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        self.train_losses.append(avg_loss)\n",
    "        return avg_loss\n",
    "\n",
    "    def validate_epoch(self, dataloader):\n",
    "        \"\"\"\n",
    "        Evaluates the model on the validation set for one epoch.\n",
    "\n",
    "        Args:\n",
    "            dataloader (DataLoader): The DataLoader for the validation set.\n",
    "\n",
    "        Returns:\n",
    "            float: The average validation loss for the epoch.\n",
    "            float: The average validation accuracy for the epoch.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        num_batches = len(dataloader)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_samples += labels.size(0)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        accuracy = correct_predictions / total_samples\n",
    "        self.val_losses.append(avg_loss)\n",
    "        self.val_accuracies.append(accuracy)\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def train(self, train_dataloader, val_dataloader, num_epochs):\n",
    "        \"\"\"\n",
    "        Trains the model for a specified number of epochs and validates it, recording latency and throughput.\n",
    "\n",
    "        Args:\n",
    "            train_dataloader (DataLoader): The DataLoader for the training set.\n",
    "            val_dataloader (DataLoader): The DataLoader for the validation set.\n",
    "            num_epochs (int): The number of training epochs.\n",
    "        \"\"\"\n",
    "        print(f\"Training on {self.device}\")\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = self.train_epoch(train_dataloader)\n",
    "            val_loss, val_acc = self.validate_epoch(val_dataloader)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    def plot_losses_accuracies(self):\n",
    "        \"\"\"\n",
    "        Plots the training loss, validation loss, and validation accuracy against the number of epochs.\n",
    "        \"\"\"\n",
    "        epochs = range(1, len(self.train_losses) + 1)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(epochs, self.train_losses, label='Train Loss')\n",
    "        plt.plot(epochs, self.val_losses, label='Validation Loss')\n",
    "        plt.plot(epochs, self.val_accuracies, label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(self.name + ' Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "trans = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "# Load the CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=trans)\n",
    "val_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=trans)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256)\n",
    "\n",
    "@torch.no_grad()\n",
    "def measure_inference_time_with_warmup(model, data_loader, warmup_steps=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    # Warm-up\n",
    "    for i, (inputs, _) in enumerate(data_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        _ = model(inputs)\n",
    "        if i >= warmup_steps - 1:\n",
    "            break\n",
    "\n",
    "    total_inference_time = 0\n",
    "    num_samples = 0\n",
    "    start_time = time.time() # Start timer after warm-up\n",
    "\n",
    "    for inputs, labels in data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        _ = model(inputs)\n",
    "        num_samples += inputs.size(0)\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_inference_time = end_time - start_time\n",
    "    throughput = num_samples / total_inference_time\n",
    "    avg_latency_per_sample = total_inference_time / num_samples\n",
    "    print(f\"Average Latency per Sample (with warm-up): {avg_latency_per_sample * 1000:.2f} ms\")\n",
    "    print(f\"Throughput (with warm-up): {throughput:.2f} samples/second\")\n",
    "    return avg_latency_per_sample, throughput\n",
    "\n",
    "def fit_one(name,model,epochs,train_dataloader,val_dataloader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    trainer = Trainer(name = name,model = model,criterion = nn.CrossEntropyLoss(),optimizer= optim.SGD(model.parameters(),weight_decay=0.0001,momentum=0.9),device=device)\n",
    "    trainer.train(train_dataloader, val_dataloader, epochs)\n",
    "    trainer.plot_losses_accuracies()\n",
    "    max_acc = np.max(trainer.val_accuracies)\n",
    "    avg_latency, throughput = measure_inference_time_with_warmup(model, val_dataloader)\n",
    "    return avg_latency,max_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69c00ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ablation Study\n",
    "model_dict = {}\n",
    "result_list = []\n",
    "model_dict['resnet14_1s1x64d'] = ResNet(Bottleneck, [1, 1, 1, 1],num_classes=10,final_drop=0.2)\n",
    "model_dict['resnest14_fast_0s1x64d'] = ResNet(Bottleneck, [1, 1, 1, 1],num_classes=10,final_drop=0.2,radix=0, groups=1, bottleneck_width=64,\n",
    "                   deep_stem=True, stem_width=32, avg_down=True,\n",
    "                   avd=True, avd_first=True)\n",
    "model_dict['resnest14_fast_1s1x64d'] = ResNet(Bottleneck, [1, 1, 1, 1],num_classes=10,final_drop=0.2,radix=1, groups=1, bottleneck_width=64,\n",
    "                   deep_stem=True, stem_width=32, avg_down=True,\n",
    "                   avd=True, avd_first=True)\n",
    "model_dict['resnest14_fast_1s1x40d'] = ResNet(Bottleneck, [1, 1, 1, 1],num_classes=10,final_drop=0.2,radix=1, groups=1, bottleneck_width=40,\n",
    "                   deep_stem=True, stem_width=32, avg_down=True,\n",
    "                   avd=True, avd_first=True)\n",
    "model_dict['resnest14_fast_1s2x64d'] = ResNet(Bottleneck, [1, 1, 1, 1],num_classes=10,final_drop=0.2,radix=1, groups=2, bottleneck_width=64,\n",
    "                   deep_stem=True, stem_width=32, avg_down=True,\n",
    "                   avd=True, avd_first=True)\n",
    "model_dict['resnet14_0s1x64d'] = ResNet(Bottleneck, [1, 1, 1, 1],num_classes=10,final_drop=0.2,radix=0)\n",
    "model_dict['resnest14_fast_2s1x64d'] = ResNet(Bottleneck, [1, 1, 1, 1],num_classes=10,final_drop=0.2,radix=2, groups=1, bottleneck_width=64,\n",
    "                   deep_stem=True, stem_width=32, avg_down=True,\n",
    "                   avd=True, avd_first=True)\n",
    "for name,model in model_dict.items():\n",
    "    avg_latency,max_acc = fit_one(name,model,20,train_dataloader,val_dataloader)\n",
    "    result_list.append((name,avg_latency,max_acc))\n",
    "print(result_list)\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Counts the total number of parameters in a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model.\n",
    "\n",
    "    Returns:\n",
    "        int: The total number of parameters.\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters())/1e6\n",
    "\n",
    "for name,model in model_dict.items():\n",
    "    print(name,count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4716ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Comparison 1\n",
    "model_dict = {}\n",
    "result_list = []\n",
    "model_dict['resnest_14'] = ResNet(Bottleneck, [1, 1, 1, 1],\n",
    "                                  radix=2, groups=1, bottleneck_width=64,\n",
    "                                  deep_stem=True, stem_width=32, avg_down=True,\n",
    "                                  avd=True, avd_first=False, num_classes=10,final_drop=0.2)\n",
    "model_dict['resnest_32'] = ResNet(Bottleneck, [2, 3, 3, 2],\n",
    "                                  radix=2, groups=1, bottleneck_width=64,\n",
    "                                  deep_stem=True, stem_width=32, avg_down=True,\n",
    "                                  avd=True, avd_first=False, num_classes=10,final_drop=0.2)\n",
    "model_dict['resnest_50'] = ResNet(Bottleneck, [3, 4, 6, 3],\n",
    "                                  radix=2, groups=1, bottleneck_width=64,\n",
    "                                  deep_stem=True, stem_width=32, avg_down=True,\n",
    "                                  avd=True, avd_first=False, num_classes=10,final_drop=0.2)\n",
    "model_dict['resnest_101'] = ResNet(Bottleneck, [3, 4, 23, 3],\n",
    "                                  radix=2, groups=1, bottleneck_width=64,\n",
    "                                  deep_stem=True, stem_width=32, avg_down=True,\n",
    "                                  avd=True, avd_first=False,num_classes=10,final_drop=0.2)\n",
    "for name,model in model_dict.items():\n",
    "    avg_latency,max_acc = fit_one(name,model,20,train_dataloader,val_dataloader)\n",
    "    result_list.append((name,avg_latency,max_acc))\n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0cd32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Comparison 2\n",
    "from torchvision.models import efficientnet_b0,efficientnet_b1,efficientnet_b2,efficientnet_b3,resnet18\n",
    "model_dict = {}\n",
    "result_list = []\n",
    "model_dict['efficientnet_b0'] = efficientnet_b0(num_classes = 10, dropout = 0.2)\n",
    "model_dict['efficientnet_b1'] = efficientnet_b1(num_classes = 10, dropout = 0.2)\n",
    "model_dict['efficientnet_b2'] = efficientnet_b2(num_classes = 10, dropout = 0.2)\n",
    "model_dict['efficientnet_b3'] = efficientnet_b3(num_classes = 10, dropout = 0.2)\n",
    "model_dict['resnet18'] = resnet18(num_classes = 10)\n",
    "for name,model in model_dict.items():\n",
    "    avg_latency,max_acc = fit_one(name,model,20,train_dataloader,val_dataloader)\n",
    "    result_list.append((name,avg_latency,max_acc))\n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a29884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from d2l import torch as d2l\n",
    "import os\n",
    "from torch import nn\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n",
    "d2l.DATA_HUB['hotdog'] = (d2l.DATA_URL + 'hotdog.zip',\n",
    "                         'fba480ffa8aa7e0febbb511d181409f899b9baa5')\n",
    "data_dir = d2l.download_extract('hotdog')\n",
    "\n",
    "train_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'train'))\n",
    "test_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'test'))\n",
    "\n",
    "\n",
    "# Specify means and standard deviations of three RGB channels to standardize each channel\n",
    "normalize = torchvision.transforms.Normalize(\n",
    "    [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "# Using the mean and std of Imagenet is a common practice. They are calculated\n",
    "# based on millions of images. If you want to train from scratch on your own\n",
    "# dataset, you can calculate the new mean and std. Otherwise, using Imagenet\n",
    "# pretrained model with its own mean and std is recommended.\n",
    "\n",
    "\n",
    "train_augs = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(), normalize])\n",
    "\n",
    "test_augs = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize([256, 256]),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(), normalize])\n",
    "\n",
    "\n",
    "# If `param_group=True`, parameters in output layer are updated using learning rate 10 times greater\n",
    "def train_fine_tuning(net, learning_rate, batch_size=64, num_epochs=5, param_group=True):\n",
    "    train_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(\n",
    "        os.path.join(data_dir, 'train'), transform=train_augs), batch_size=batch_size, shuffle=True)\n",
    "    test_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(\n",
    "        os.path.join(data_dir, 'test'), transform=test_augs), batch_size=batch_size)\n",
    "    devices = d2l.try_all_gpus()\n",
    "    loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    if param_group:\n",
    "        params_1x = [param for name, param in net.named_parameters() # all parameters not in output layer\n",
    "             if name not in [\"fc.weight\", \"fc.bias\"]]\n",
    "        trainer = torch.optim.SGD([{'params': params_1x}, {'params': net.fc.parameters(),\n",
    "                  'lr': learning_rate * 10}], lr=learning_rate, weight_decay=0.001)\n",
    "    else:\n",
    "        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\n",
    "\n",
    "def transform_fc_train(model):\n",
    "    model.fc = nn.Linear(model.fc.in_features, 2)   # change no. of classes to 2\n",
    "    nn.init.xavier_uniform_(model.fc.weight)\n",
    "    train_fine_tuning(model, 5e-5)\n",
    "# Fine-tuning\n",
    "# using ResNeSt-50 as an example\n",
    "# get list of models\n",
    "torch.hub.list('zhanghang1989/ResNeSt', force_reload=True)\n",
    "\n",
    "# load pretrained models, using ResNeSt-50 as an example\n",
    "resnest50_pretrained = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)\n",
    "transform_fc_train(resnest50_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0135f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50,ResNet50_Weights\n",
    "resnet50_pretrained = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "transform_fc_train(resnet50_pretrained)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
